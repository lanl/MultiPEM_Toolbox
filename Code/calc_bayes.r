########################################################################
#                                                                      #
# This file contains code for maximizing and sampling the log-         #
# posterior of the calibration parameters, with an option to include   #
# errors-in-variables for calibration data yields. New event data can  #
# also be optionally incorporated, allowing for joint inference of new #
# event and calibration parameters based on calibration and new event  #
# data.                                                                #
#                                                                      #
# Â© 2023. Triad National Security, LLC. All rights reserved.           #
# This program was produced under U.S. Government contract             #
# 89233218CNA000001 for Los Alamos National Laboratory (LANL), which   #
# is operated by Triad National Security, LLC for the U.S. Department  #
# of Energy/National Nuclear Security Administration. All rights in    #
# the program are reserved by Triad National Security, LLC, and the    #
# U.S. Department of Energy/National Nuclear Security Administration.  #
# The Government is granted for itself and others acting on its behalf #
# a nonexclusive, paid-up, irrevocable worldwide license in this       #
# material to reproduce, prepare derivative works, distribute copies   #
# to the public, perform publicly and display publicly, and to permit  #
# others to do so.                                                     #
#                                                                      #
########################################################################

calc_bayes = function(p_cal,gdir,adir,nst=10,nburn=10000,nmcmc=20000,
                      nthin=20,ncor_map=1,ncor_mc=1,igrad=TRUE,
                      igrck_pr=TRUE,igrck_po=TRUE,bfgs=TRUE,ibpr=FALSE,
                      icpr=FALSE,itpr=FALSE,fpr_b=NULL,fgpr_b=NULL,
                      fpr_c=NULL,fgpr_c=NULL,fpr_t=NULL,fgpr_t=NULL,
                      Xnom=NULL,imcmc="FME",pl="multicore",t_cal=NULL)
{
  #
  # FUNCTION INPUTS
  #

  # p_cal: environment storing all objects needed in characterization
  #        calculations
  # gdir: directory for general subroutines
  # adir: directory for application subroutines
  # nst: number of starting values for MAP optimization
  # nburn: number of Markov chain Monte Carlo (MCMC) burn-in samples
  # nmcmc: number of MCMC production samples
  # nthin: posterior sample thinning rate
  # ncor_map: number of cores for MAP optimization
  # ncor_mc: number of cores for generating parallel MCMC chains
  # igrad: forward model gradients provided (TRUE/FALSE)
  # igrck_pr: Prior distribution gradient check (TRUE/FALSE)
  # igrck_po: Posterior distribution gradient check (TRUE/FALSE)
  # bfgs: MAP optimization uses BFGS methods (TRUE/FALSE)
  # ibpr: prior distributions provided for forward model
  #       coefficients (TRUE/FALSE)
  # icpr: prior distributions provided for calibration inference
  #       parameters (TRUE/FALSE)
  # itpr: prior distributions provided for new event
  #       parameters (TRUE/FALSE)
  # fpr_b: location of functions computing log-prior density for
  #        forward model coefficients
  # fgpr_b: location of functions computing gradients of log-prior
  #         density for forward model coefficients
  # fpr_c: location of functions computing log-prior density for
  #        calibration inference parameters
  # fgpr_c: location of functions computing gradients of log-prior
  #         density for calibration inference parameters
  # fpr_t: location of functions computing log-prior density for
  #        new event parameters
  # fgpr_t: location of functions computing gradients of log-prior
  #         density for new event parameters
  # Xnom: matrix of starting values for hyperparameters in MAP
  #       optimization if not generated by this function
  # imcmc: MCMC algorithm (current options:  "RAM", "FME", "NUTS")
  # pl: strategy for running parallel jobs (see help for plan()
  #     function in future package)
  # t_cal: object used if bounds supplied to MLE optimization

  #
  # END FUNCTION INPUTS
  #

  #   
  # SOURCE SUPPORTING R FUNCTIONS
  #     

  if( !exists("obs_info_0",where=p_cal,inherits=FALSE) ){
    # R function to calculate observed information matrix
    source(paste(gdir,"/observed_information_0.r",sep=""),local=TRUE)
    p_cal$obs_info_0 = obs_info_0
  }

  #   
  # END SOURCE SUPPORTING R FUNCTIONS
  #

  #
  # BAYESIAN ANALYSIS
  #

  if( ibpr ){
    # prior distributions for forward model coefficients
    if( is.null(fpr_b) ){
      stop("log-prior density files must be provided: forward model")
    }
    for( qq in 1:length(fpr_b) ){ source(fpr_b[qq],local=TRUE) }
    for( hh in 1:p_cal$H ){
      pnames = names(p_cal$h[[hh]])
      if( "lp_beta" %in% pnames ){
        ufn = unique(p_cal$h[[hh]]$lp_beta$f)
        lfn = length(ufn)
        for( qq in 1:lfn ){
          eval(parse(text=paste("p_cal$flp$",ufn[qq]," = ",ufn[qq],
                                sep="")))
        }
      }
      if( "lp_betat" %in% pnames ){
        for( tt in 1:p_cal$h[[hh]]$Th ){
          if( !is.null(p_cal$h[[hh]]$lp_betat[[tt]]) ){
            ufn = unique(p_cal$h[[hh]]$lp_betat[[tt]]$f)
            lfn = length(ufn)
            for( qq in 1:lfn ){
              eval(parse(text=paste("p_cal$flp$",ufn[qq]," = ",ufn[qq],
                                    sep="")))
            }
          }
        }
      }
    }
    # gradients
    if( igrad ){
      if( is.null(fgpr_b) ){
        stop(paste("gradient log-prior density files must be provided:",
                   " forward model",sep=""))
      }
      for( qq in 1:length(fgpr_b) ){ source(fgpr_b[qq],local=TRUE) }
      for( hh in 1:p_cal$H ){
        pnames = names(p_cal$h[[hh]])
        if( "lp_beta" %in% pnames ){
          ugn = unique(p_cal$h[[hh]]$lp_beta$g)
          lgn = length(ugn)
          for( qq in 1:lgn ){
            eval(parse(text=paste("p_cal$glp$",ugn[qq]," = ",ugn[qq],
                                  sep="")))
          }
        }
        if( "lp_betat" %in% pnames ){
          for( tt in 1:p_cal$h[[hh]]$Th ){
            if( !is.null(p_cal$h[[hh]]$lp_betat[[tt]]) ){
              ugn = unique(p_cal$h[[hh]]$lp_betat[[tt]]$g)
              lgn = length(ugn)
              for( qq in 1:lgn ){
                eval(parse(text=paste("p_cal$glp$",ugn[qq]," = ",
                                      ugn[qq],sep="")))
              }
            }
          }
        }
      }
    }
  }

  if( exists("nev",where=p_cal,inherits=FALSE) && p_cal$nev && itpr ){
    # prior distributions for new event parameters
    if( is.null(fpr_t) ){
      source(paste(adir,"/lp_0.r",sep=""),local=TRUE)
    } else { source(fpr_t,local=TRUE) }
    if( "lp_theta0" %in% names(p_cal) ){
      ufn = p_cal$lp_theta0$f
      eval(parse(text=paste("p_cal$flp$",ufn," = ",ufn,sep="")))
    }
    # gradients
    if( igrad ){
      if( is.null(fgpr_t) ){
        source(paste(adir,"/glp_0.r",sep=""),local=TRUE)
      } else { source(fgpr_t,local=TRUE) }
      if( "lp_theta0" %in% names(p_cal) ){
        ugn = p_cal$lp_theta0$g
        eval(parse(text=paste("p_cal$glp$",ugn," = ",ugn,sep="")))
      }
    }
  }

  if( icpr && p_cal$ncalp > 0 ){
    # prior distributions for calibration inference parameters
    if( is.null(fpr_c) ){
      source(paste(adir,"/lp_c.r",sep=""),local=TRUE)
    } else { source(fpr_c,local=TRUE) }
    if( "lp_calp" %in% names(p_cal) ){
      ufn = p_cal$lp_calp$f
      eval(parse(text=paste("p_cal$flp$",ufn," = ",ufn,sep="")))
    }
    # gradients
    if( igrad ){
      if( is.null(fgpr_c) ){
        source(paste(adir,"/glp_c.r",sep=""),local=TRUE)
      } else { source(fgpr_c,local=TRUE) }
      if( "lp_calp" %in% names(p_cal) ){
        ugn = p_cal$lp_calp$g
        eval(parse(text=paste("p_cal$glp$",ugn," = ",ugn,sep="")))
      }
    }
  }

  # FGSN parameters for errors-in-variables yields prior
  # total number of FGSN parameters
  p_cal$p_fgsn = 0
  if( exists("eiv",where=p_cal,inherits=FALSE) && p_cal$eiv ){
    # number of components
    if( !exists("K",where=p_cal,inherits=FALSE) ){ p_cal$K = 2 }
    p_cal$p_fgsn = p_cal$K + 2
  }

  # Scale parameters for variance component priors
  # total number of scale parameters
  p_cal$p_A = 0
  if( !exists("A",where=p_cal,inherits=FALSE) ){
    for( hh in 1:p_cal$H ){
      if( p_cal$pvc_1 > 0 ){
        p_cal$p_A = p_cal$p_A + sum(p_cal$h[[hh]]$pvc_1 > 0)
      } else if( p_cal$pvc_2 > 0 ){
        p_cal$p_A = p_cal$p_A + sum(p_cal$h[[hh]]$pvc_2 > 0)
      }
    }
  }

  # total number of parameters (model + prior)
  npars = p_cal$nmpars + p_cal$p_fgsn + p_cal$p_A

  # starting values for parameters in MAP optimization
  if( exists("nev",where=p_cal,inherits=FALSE) && p_cal$nev ){
    tlb = p_cal$theta0_bounds[,1]
    tub = p_cal$theta0_bounds[,2]
  } 
  Xst = matrix(p_cal$mle,nrow=1)
  p_cal$npars = p_cal$nmpars
  if( nst != nrow(p_cal$Xst) ){ nst = nrow(p_cal$Xst) }
  if( nst > 1 ){ Xst = rbind(Xst,p_cal$Xst[2:nst,,drop=FALSE]) }
  if( npars > p_cal$nmpars ){
    p_cal$npars = npars
    if( is.null(Xnom) ){
      Xnom = numeric(npars-p_cal$nmpars)
      if( nst > 1 ){
        for( ii in 2:nst ){
          Xnom = rbind(Xnom,runif(npars-p_cal$nmpars,min=-2,max=2))
        }
      }
    } else { Xnom = Xnom }
    Xst = cbind(Xst,Xnom)
  }

  # Source log-prior and gradient of the log-prior
  source(paste(gdir,"/log_prior.r",sep=""),local=TRUE)
  if( igrad ){ source(paste(gdir,"/glog_prior.r",sep=""),local=TRUE) }

  # Attach prior functions needed for Bayesian computation
  if( !igrad ){ glprior = NULL }
  p_cal$lprior = lprior; p_cal$k_ij = k_ij;
  p_cal$glprior = glprior
  if( exists("eiv",where=p_cal,inherits=FALSE) && p_cal$eiv ){
    if( !igrad ){ gdfgsn = NULL }
    p_cal$dfgsn = dfgsn; p_cal$dinvgamma = dinvgamma;
    p_cal$gdfgsn = gdfgsn
  }
  if( p_cal$pvc_1 > 0 || p_cal$pvc_2 > 0 ){ p_cal$lphc = lphc }

  # Source log-posterior and gradient of the log-posterior
  source(paste(gdir,"/log_posterior_full.r",sep=""),local=TRUE)
  if( igrad ){
    source(paste(gdir,"/glog_posterior_full.r",sep=""),local=TRUE)
  }

  # Attach posterior functions needed for Bayesian computation
  if( !igrad ){ glpost_full = NULL }
  p_cal$lpost_full = lpost_full; p_cal$glpost_full = glpost_full;

  # Maximize log posterior to find MAP estimate
  if( p_cal$opt_B ){
    lb_optim = rep(-Inf, npars)
    if( exists("nev",where=p_cal,inherits=FALSE) && p_cal$nev ){
      lb_optim[1:p_cal$ntheta0] = tlb
    }
    p_cal$lb_optim = lb_optim
    ub_optim = rep(Inf, npars)
    if( exists("nev",where=p_cal,inherits=FALSE) && p_cal$nev ){
      ub_optim[1:p_cal$ntheta0] = tub
    }
    p_cal$ub_optim = ub_optim
  }
  for( ii in 1:nst ){
    xst = Xst[ii,]
    while( is.infinite(lpost_full(xst, pc=p_cal)) ){
      xst = calc_xst(p_cal,t_cal)
    }
    Xst[ii,] = xst
  }
  p_cal$calc_xst = calc_xst
  # parallel optimization using R package "future"
  require(doFuture)
  if( ncor_map == 1 ){ pl = "sequential"; plan(pl);
  } else {
    if( ncor_map > nst ){ ncor_map = nst }
    if( pl != "sequential" ){ plan(pl,workers=ncor_map)
    } else { plan(pl) }
  }
  cmax = -Inf
  ptm = proc.time()
  fCatch = foreach( qq = 1:nst ) %dofuture% {
             lpo_opt(Xst[qq,],bfgs,p_cal)
           } %seed% TRUE
  plan(sequential)
  print("Run time:")
  print(proc.time() - ptm)
  cat("\n")
  Mlp = NULL
  for( qq in 1:nst ){
    if( is(fCatch[[qq]]$value,"list") ){
      Map = fCatch[[qq]]$value
      Mlp = c(Mlp,Map$value)
    } else { Mlp = c(Mlp,-Inf) }
  }
  imax = which( Mlp == max(Mlp) ); imax = imax[1];
  if( Mlp[imax] > cmax ){
    cmax = Mlp[imax]
    map = fCatch[[imax]]$value
  }
  qq = 0; irel_tol = 0;
  while( qq <= 5 ){
    cmax = map$value
    if( bfgs ){
      if( p_cal$opt_B ){
        map = optim(map$par, fn=lpost_full, gr=glpost_full, pc=p_cal,
                    method="L-BFGS-B",
                    lower=lb_optim, upper=ub_optim,
                    control=list(fnscale = -1,maxit=1000))
      } else {
        map = optim(map$par, fn=lpost_full, gr=glpost_full, pc=p_cal,
                    method="BFGS",
                    control=list(fnscale = -1,maxit=1000))
      }
    } else {
      if( p_cal$opt_B ){
        map = optim(map$par, fn=lpost_full, pc=p_cal,
                    lower=lb_optim, upper=ub_optim,
                    control=list(fnscale = -1,maxit=10000))
      } else {
        map = optim(map$par, fn=lpost_full, pc=p_cal,
                    control=list(fnscale = -1,maxit=10000))
      }
    }
    rel_tol = abs(map$value - cmax)/abs(cmax)
    if( rel_tol <= 1.e-8 ){ irel_tol = irel_tol+1
    } else { qq = qq+1 }
    if( irel_tol == 2 ){ break }
  }
  if( imcmc %in% c("RAM","FME") ){
    # inverse Hessian for starting proposal distribution in
    # adaptive MCMC
    if( bfgs ){
      if( p_cal$opt_B ){
        map = optim(map$par, fn=lpost_full, gr=glpost_full, pc=p_cal,
                    method="L-BFGS-B",
                    lower=lb_optim, upper=ub_optim,
                    control=list(fnscale = -1,maxit=1000),
                    hessian=TRUE)
      } else {
        map = optim(map$par, fn=lpost_full, gr=glpost_full, pc=p_cal,
                    method="BFGS",
                    control=list(fnscale = -1,maxit=1000),
                    hessian=TRUE)
      }
    } else {
      if( p_cal$opt_B ){
        map = optim(map$par, fn=lpost_full, pc=p_cal,
                    lower=lb_optim, upper=ub_optim,
                    control=list(fnscale = -1,maxit=10000),
                    hessian=TRUE)
      } else {
        map = optim(map$par, fn=lpost_full, pc=p_cal,
                    control=list(fnscale = -1,maxit=10000),
                    hessian=TRUE)
      }
    }
    if( p_cal$opt_B ){
      if( is.null(t_cal) ){
        stop("List t_cal must be provided for bounded optimization.")
      }
      p_cal = p_cal$obs_info_0(p_cal, map, imle=FALSE, t_cal=t_cal)
    } else { p_cal = p_cal$obs_info_0(p_cal, map, imle=FALSE) }
    lambda = (2.38)^2/npars
    p_cal$IHess = lambda*p_cal$IHess
  }

  # Print convergence status of log-posterior optimization
  print("MAP CONVERGENCE STATUS")
  cat("\n")
  print(map$convergence)
  print(irel_tol)

  # Additional information if convergence obtained
  if( map$convergence == 0 ){
    p_cal$map = unname(map$par)
    # Print MAP
    print("MAXIMUM A POSTERIORI SUMMARY")
    cat("\n")
    # Indicator of prior distribution parameters
    p_cal$iPrior = TRUE
    p_cal = p_cal$print_ss(map$par, p_cal)
  }

  # Set up unbounded parameters for posterior sampling
  if( p_cal$opt_B ){
    if( is.null(t_cal) ){
      stop("List t_cal must be provided for bounded optimization.")
    }
    p_cal$itheta0_bounds = t_cal$itheta0_bounds
    map$par[1:p_cal$ntheta0] =
      p_cal$inv_transform(map$par[1:p_cal$ntheta0],pc=p_cal)
    if( exists("itransform",where=p_cal,inherits=FALSE) ){
      if( p_cal$itransform ){
        map$par[1:p_cal$ntheta0] =
          p_cal$inv_tau(map$par[1:p_cal$ntheta0],pc=p_cal)
      }
    }
  }

  # Check log-prior gradient calculations
  if( igrck_pr && igrad ){
    # numerical differentiation package
    require(numDeriv)
    print("CHECK LOG-PRIOR GRADIENTS")
    cat("\n")
    xnom = unname(map$par)
    Catch = p_cal$tryCatch.W.E(glprior(xnom,p_cal))
    if( is(Catch$value,"numeric") ){ glpr_nom = Catch$value
    } else {
      glpr_nom = NaN
      print("Error computing analytic gradient at MAP.")
    }
    Catch = p_cal$tryCatch.W.E(grad(lprior, xnom, method.args=list(r=6),
                                    pc=p_cal))
    if( is(Catch$value,"numeric") ){ glpr_nom_num = Catch$value
    } else {
      glpr_nom_num = NaN
      print("Error computing numerical gradient at MAP.")
    }
    diff <- glpr_nom - glpr_nom_num
    print("Analytic gradient")
    print(glpr_nom)
    print("Numerical gradient")
    print(glpr_nom_num)
    print("Difference")
    print(c(min(diff),max(diff)))
    cat("\n")

    nsamp = 5
    qq = 1
    while( qq <= nsamp ){
      xx = xnom + rnorm(npars,mean=0,sd=0.1)
      Catch = p_cal$tryCatch.W.E(glprior(xx,p_cal))
      if( is(Catch$value,"numeric") ){ glpr_xx = Catch$value
      } else { next }
      Catch = p_cal$tryCatch.W.E(grad(lprior, xx, method.args=list(r=6),
                                      pc=p_cal))
      if( is(Catch$value,"numeric") ){
        qq = qq+1
        glpr_xx_num = Catch$value
      } else { next }
      diff <- glpr_xx - glpr_xx_num
      print("Analytic gradient")
      print(glpr_xx)
      print("Numerical gradient")
      print(glpr_xx_num)
      print("Difference")
      print(c(min(diff),max(diff)))
      cat("\n")
    }
  }

  # Check log-posterior gradient calculations
  if( igrck_po && igrad ){
    # numerical differentiation package
    require(numDeriv)
    print("CHECK LOG-POSTERIOR GRADIENTS")
    cat("\n")
    xnom = unname(map$par)
    Catch = p_cal$tryCatch.W.E(glpost_full(xnom,p_cal))
    if( is(Catch$value,"numeric") ){ glpo_full_nom = Catch$value
    } else {
      glpo_full_nom = NaN
      print("Error computing analytic gradient at MAP.")
    }
    Catch = p_cal$tryCatch.W.E(grad(lpost_full, xnom,
                                    method.args=list(r=6),pc=p_cal))
    if( is(Catch$value,"numeric") ){ glpo_full_nom_num = Catch$value
    } else {
      glpo_full_nom_num = NaN
      print("Error computing numerical gradient at MAP.")
    }
    diff <- glpo_full_nom - glpo_full_nom_num
    print("Analytic gradient")
    print(glpo_full_nom)
    print("Numerical gradient")
    print(glpo_full_nom_num)
    print("Difference")
    print(c(min(diff),max(diff)))
    cat("\n")

    nsamp = 5
    qq = 1
    while( qq <= nsamp ){
      xx = xnom + rnorm(npars,mean=0,sd=0.1)
      Catch = p_cal$tryCatch.W.E(glpost_full(xx,p_cal))
      if( is(Catch$value,"numeric") ){ glpo_full_xx = Catch$value
      } else { next }
      Catch = p_cal$tryCatch.W.E(grad(lpost_full, xx,
                                      method.args=list(r=6),pc=p_cal))
      if( is(Catch$value,"numeric") ){
        qq = qq+1
        glpo_full_xx_num = Catch$value
      } else { next }
      diff <- glpo_full_xx - glpo_full_xx_num
      print("Analytic gradient")
      print(glpo_full_xx)
      print("Numerical gradient")
      print(glpo_full_xx_num)
      print("Difference")
      print(c(min(diff),max(diff)))
      cat("\n")
    }
  }

  if (imcmc == "RAM"){
    # Robust Adaptive Metropolis sampling
    # parallel chains using R package "future"
    require(future)
    # using R package "adaptMCMC"
    require(adaptMCMC)
    if( ncor_mc == 1 ){ pl = "sequential"; plan(pl);
    } else {
      if( pl != "sequential" ){ plan(pl,workers=ncor_mc)
      } else { plan(pl) }
    }
    xi = NULL
    if( ncor_mc > 1 ){
      eps = 0.001
      for (qq in 1:ncor_mc){
        xi = rbind(xi, map$par*(1+runif(length(map$par),-eps,eps)))
      }
    } else { xi = rbind(xi, map$par) }
    si = sample(10000,ncor_mc)
    ptm = proc.time()
    fram = lapply(1:ncor_mc, function(qq) { future(MCMC(lpost_full,
                             nburn+round(nmcmc/ncor_mc),init=xi[qq,],
                             scale = as.matrix(p_cal$IHess),
                             acc.rate=0.234,showProgressBar=FALSE,
                             pc=p_cal),seed=si[qq]) })
    ram = lapply(fram,value)
    plan(sequential)
    print("Run time:")
    print(proc.time() - ptm)
    cat("\n")
    # acceptance rate
    print("ACCEPTANCE RATES:")
    cat("\n")
    for( qq in 1:ncor_mc ){
      print(paste("Core ",qq,": ",ram[[qq]]$acceptance.rate,sep=""))
    }
    cat("\n")
    # extract post-burnin posterior samples
    mpi = NULL
    for (qq in 1:ncor_mc){
      tmpi = ram[[qq]]$samples
      mpi = rbind(mpi,as.matrix(tmpi[-(1:nburn),]))
    }
  }
  if (imcmc == "FME"){
    # Constrained MCMC Sampler
    # parallel chains using R package "future"
    require(future)
    # using R package "FME"
    require(FME)
    if( ncor_mc == 1 ){ pl = "sequential"; plan(pl);
    } else { 
      if( pl != "sequential" ){ plan(pl,workers=ncor_mc)
      } else { plan(pl) }
    }
    xi = NULL
    if( ncor_mc > 1 ){
      eps = 0.001
      for (qq in 1:ncor_mc){
        xi = rbind(xi, map$par*(1+runif(length(map$par),-eps,eps)))
      }
    } else { xi = rbind(xi, map$par) }
    si = sample(10000,ncor_mc)
    ll_full_mod = function(x, pc=p_cal){ -2*pc$ll_full(x, pc) }
    lprior_mod = function(x, pc=p_cal){ -2*pc$lprior(x, pc) }
    ptm = proc.time()
    ffme = lapply(1:ncor_mc, function(qq) { future(modMCMC(
                             ll_full_mod,xi[qq,],pc=p_cal,
                             jump=as.matrix(p_cal$IHess),
                             lower=rep(-Inf,npars),upper=rep(Inf,npars),
                             prior=lprior_mod,
                             niter=nburn+round(nmcmc/ncor_mc),
                             burninlength=nburn,updatecov=100,ntrydr=2,
                             verbose=FALSE),seed=si[qq]) })
    fme = lapply(ffme,value)
    plan(sequential)
    print("Run time:")
    print(proc.time() - ptm)
    cat("\n")
    # acceptance rate
    print("ACCEPTANCE RATES:")
    cat("\n")
    for( qq in 1:ncor_mc ){
      print(paste("Core ",qq,": ",fme[[qq]]$naccepted/
                                  (nburn+round(nmcmc/ncor_mc)),sep=""))
    }
    cat("\n")
    # extract post-burnin posterior samples
    mpi = NULL
    for (qq in 1:ncor_mc){
      tmpi = fme[[qq]]$pars
      mpi = rbind(mpi,tmpi)
    }
    if (is.vector(mpi)) { mpi = matrix(mpi,ncol=1) }
  }
  if (imcmc == "NUTS"){
    # Hamiltonian Monte Carlo No-U-Turn (NUTS) sampling
    # parallel chains using R package "future"
    require(future)
    # using R code from https://github.com/kasparmartens/NUTS
    source(paste(gdir,"/helpers.r",sep=""),local=TRUE)
    source(paste(gdir,"/nuts.r",sep=""),local=TRUE)
    if( ncor_mc == 1 ){ pl = "sequential"; plan(pl);
    } else { 
      if( pl != "sequential" ){ plan(pl,workers=ncor_mc)
      } else { plan(pl) }
    }
    xi = NULL
    if( ncor_mc > 1 ){
      eps = 0.001
      for (qq in 1:ncor_mc){
        xi = rbind(xi, map$par*(1+runif(length(map$par),-eps,eps)))
      }
    } else { xi = rbind(xi, map$par) }
    si = sample(10000,ncor_mc)
    if( !igrad ){
      stop("Gradient of log-posterior must be provided.")
    }
    ptm = proc.time()
    fham = lapply(1:ncor_mc, function(qq) { future(NUTS(xi[qq,],
                             f=lpost_full,grad_f=glpost_full,
                             n_iter=nburn+round(nmcmc/ncor_mc),
                             delta=0.8,verbose=FALSE),
                             seed=si[qq]) })
    ham = lapply(fham, value)
    plan(sequential)
    print("Run time:")
    print(proc.time() - ptm)
    cat("\n")
    # extract post-burnin posterior samples
    mpi = NULL
    for (qq in 1:ncor_mc){
      tmpi = ham[[qq]]
      mpi = rbind(mpi, as.matrix(tmpi[-(1:nburn),]))
    }
  }

  # Print posterior summary statistics
  print("POSTERIOR SUMMARY")
  cat("\n")
  # Indicator of prior distribution parameters
  p_cal$iPrior = TRUE
  p_cal = p_cal$print_ss(mpi,p_cal,levels=c(0.025,0.05,0.5,0.95,0.975))

  # Model summaries
  ipi = seq(1,nmcmc,by=nthin)
  mpi_t = mpi[ipi,]
  if( is.vector(mpi_t) ){
    mpi_t = matrix(mpi_t,ncol=1)
  }
  tnmcmc = nrow(mpi_t)
  # compute log-likelihood of thinned posterior samples
  mpi_t_llik = NULL
  for( qq in 1:tnmcmc ){
    mpi_t_llik = c(mpi_t_llik,
                   p_cal$ll_full(mpi_t[qq,],p_cal))
  }
  mpi_t_ellik = mean(mpi_t_llik)
  # compute effective number of parameters
  Pd = 2*p_cal$ll_full(apply(mpi_t,2,mean),p_cal)-2*mpi_t_ellik
  # compute DIC and Ando (2011) IC (PIC)
  DIC = -2*mpi_t_ellik+Pd
  PIC = DIC+Pd

  print(paste("DIC = ",round(DIC,2),sep=""))
  cat("\n")
  print(paste("PIC = ",round(PIC,2),sep=""))
  cat("\n")

  #
  # END BAYESIAN ANALYSIS
  #

  return(p_cal)
}

calc_xst = function(p_cal,t_cal)
{
  nxst = 0
  xst = NULL
  if( exists("nev",where=p_cal,inherits=FALSE) && p_cal$nev ){
    xst = runif(p_cal$ntheta0, min=-2, max=2)
    tlb = p_cal$theta0_bounds[,1]
    tub = p_cal$theta0_bounds[,2]
    itheta0_bounds = vector("list",3)
    if( exists("itheta0_bounds",where=p_cal,inherits=FALSE) ){
      itheta0_bounds=p_cal$itheta0_bounds
    } else if( !is.null(t_cal) ){
      itheta0_bounds=t_cal$itheta0_bounds
    }
    if( length(itheta0_bounds[[1]]) > 0 ){
      for( jj in itheta0_bounds[[1]] ){
        xst[jj] = runif(1, min=tlb[jj],max=tlb[jj]+4)
      }
    }
    if( length(itheta0_bounds[[2]]) > 0 ){
      for( jj in itheta0_bounds[[2]] ){
        xst[jj] = runif(1, min=tub[jj]-4,max=tub[jj])
      }
    }
    if( length(itheta0_bounds[[3]]) > 0 ){
      for( jj in itheta0_bounds[[3]] ){
        xst[jj] = runif(1, min=tlb[jj],max=tub[jj])
      }
    }
    nxst = nxst + p_cal$ntheta0
  }
  xst = c(xst, runif(p_cal$nmpars-p_cal$ntheta0, min=-2, max=2))
  if( exists("nev",where=p_cal,inherits=FALSE) && p_cal$nev &&
      !p_cal$opt_B ){
    if( exists("itheta0_bounds",where=p_cal,inherits=FALSE) ){
      xst[1:p_cal$ntheta0] =
        p_cal$inv_transform(xst[1:p_cal$ntheta0],pc=p_cal)
    }
    if( exists("itransform",where=p_cal,inherits=FALSE) ){
      if( p_cal$itransform ){
        xst[1:p_cal$ntheta0] = p_cal$inv_tau(xst[1:p_cal$ntheta0],
                                             pc=p_cal)
      }
    }
  }
  if( p_cal$ncalp > 0 ){ nxst = nxst + p_cal$ncalp }
  if( exists("eiv",where=p_cal,inherits=FALSE) && p_cal$eiv ){
    xst[nxst + 1:p_cal$nsource] = p_cal$eiv_w
  }
  return(xst)
}

lpo_opt = function(xst,bfgs,pc,tc)
{
  maxiter = 5
  ii = 1
  while( ii <= maxiter ){
    if( bfgs ){
      if( pc$opt_B ){
        Catch = pc$tryCatch.W.E(optim(xst, fn=pc$lpost_full,
                gr=pc$glpost_full, pc=pc, method="L-BFGS-B",
                lower=pc$lb_optim, upper=pc$ub_optim,
                control=list(fnscale = -1,maxit=1000)))
      } else {
        Catch = pc$tryCatch.W.E(optim(xst, fn=pc$lpost_full,
                gr=pc$glpost_full, pc=pc, method="BFGS",
                control=list(fnscale = -1,maxit=1000)))
      }
    } else {
      if( pc$opt_B ){
        Catch = pc$tryCatch.W.E(optim(xst, fn=pc$lpost_full,
                pc=pc, lower=pc$lb_optim, upper=pc$ub_optim,
                control=list(fnscale = -1,maxit=10000)))
      } else {
        Catch = pc$tryCatch.W.E(optim(xst, fn=pc$lpost_full,
                pc=pc, control=list(fnscale = -1,maxit=10000)))
      }
    }
    if( is(Catch$value,"list") ){
      return(Catch)
    } else {
      xst = pc$calc_xst(pc,tc)
      while( is.infinite(pc$lpost_full(xst, pc=pc)) ){
        xst = pc$calc_xst(pc,tc)
      }
      ii = ii+1
      if( ii > maxiter ){ return(Catch) } else { next }
    }
  }
}
